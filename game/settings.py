# Game Setup
WIDTH    = 1280	
HEIGHT   = 720
FPS      = 60
TILESIZE = 64
HITBOX_OFFSET = {
    'player': -26,
    'object': -40,
    'grass': -10,
    'page': -10,
    'invisible': 0
}

# ui 
BAR_HEIGHT = 20
HEALTH_BAR_WIDTH = 200
ENERGY_BAR_WIDTH = 140
ITEM_BOX_SIZE = 80
UI_FONT = 'graphics/font/joystix.ttf'
UI_FONT_SIZE = 18

# general colors
WATER_COLOR = '#71ddee'
UI_BG_COLOR = '#222222'
UI_BORDER_COLOR = '#111111'
TEXT_COLOR = '#EEEEEE'

# ui colors
HEALTH_COLOR = 'red'
ENERGY_COLOR = 'blue'
UI_BORDER_COLOR_ACTIVE = 'gold'

# upgrade menu
TEXT_COLOR_SELECTED = '#111111'
BAR_COLOR = '#EEEEEE'
BAR_COLOR_SELECTED = '#111111'
UPGRADE_BG_COLOR_SELECTED = '#EEEEEE'
RED = '#FF0000'

# weapons 
weapon_data = {
	'sword': {'cooldown': 100, 'damage': 15,'graphic':'graphics/weapons/sword/full.png'},
	'lance': {'cooldown': 400, 'damage': 30,'graphic':'graphics/weapons/lance/full.png'},
	'axe': {'cooldown': 300, 'damage': 20, 'graphic':'graphics/weapons/axe/full.png'},
	'rapier':{'cooldown': 50, 'damage': 8, 'graphic':'graphics/weapons/rapier/full.png'},
	'sai':{'cooldown': 80, 'damage': 10, 'graphic':'graphics/weapons/sai/full.png'}}

# magic
magic_data = {
	'flame': {'strength': 5,'cost': 20,'graphic':'graphics/particles/flame/fire.png'},
	'heal' : {'strength': 20,'cost': 10,'graphic':'graphics/particles/heal/heal.png'}}

# enemy
monster_data = {
	'squid': {'health': 100,'exp':100,'damage':20,'attack_type': 'slash', 'attack_sound':'audio/attack/slash.wav', 'speed': 3, 'resistance': 3, 'attack_radius': 80, 'notice_radius': 360},
	'raccoon': {'health': 300,'exp':250,'damage':40,'attack_type': 'claw',  'attack_sound':'audio/attack/claw.wav','speed': 2, 'resistance': 3, 'attack_radius': 120, 'notice_radius': 400},
	'spirit': {'health': 100,'exp':110,'damage':8,'attack_type': 'thunder', 'attack_sound':'audio/attack/fireball.wav', 'speed': 4, 'resistance': 3, 'attack_radius': 60, 'notice_radius': 350},
	'bamboo': {'health': 70,'exp':120,'damage':6,'attack_type': 'leaf_attack', 'attack_sound':'audio/attack/slash.wav', 'speed': 3, 'resistance': 3, 'attack_radius': 50, 'notice_radius': 300}}

# Saved Files
saved_files = {
    'save1' : 'Model Deployment',
    'save2' : 'Empty Save',
    'save3' : 'Empty Save'
}

# Controls
controls = {
    'Action' : 'Controls',
    'Movement' : 'Use Arrows Keys to Move',
    'Weapon Attack' : 'Press Spacebar',
    'Magic Attack' : 'Press Left Control',
    'Switch Weapon' : 'Press Q',
    'Switch Magic' : 'Press E',
    'Close Game' : 'Press Esc',
    'Collect Tome of Knowledge' : 'Press Spacebar',
    'Toggle Tome of Knowledge' : 'Press K',
    'Toggle Control' : 'Press C',
    'Toggle Upgrade Menu' : 'Press M',
    'Toggle Test' : 'Press T',
    'Select Upgrade Attribute' : 'Press Spacebar'
}

# Buttons + Chatbot
BUTTON_WIDTH, BUTTON_HEIGHT = 150, 50
BUTTON_MARGIN = 20

# Tome
PAGE_WIDTH = 1024
PAGE_HEIGHT = 576
MAX_PAGES = 56

# Tutorial
control_rules = "Say the following and keep it short: These are the controls for the game, take some time to get familiar with them."
find_rules = "Say the following and keep it short:To get you started, we have placed 2 pages around the island. Go find them! Use this time to get familiar with the controls as well!"
found_rules = "Say the following and keep it short: Alright! You found the pages, press K to see the pages. Once you are ready to start the next day press Enter!"

### LLM ###
MAX_TOKENS = 80
MAX_CONTEXT_QUESTIONS = 10
local_models = {"tinyv1": ["../../models/tinyllama-1.1b-chat-v1.0.Q5_K_M.gguf", "HuggingFaceH4/zephyr-7b-beta", ["<|system|>", "<|user|>", "<|assistant|>", "</s>"]], 
                "tinyopenorca": ["../../models/tinyllama-1.1b-1t-openorca.Q5_K_M.gguf", "HuggingFaceH4/zephyr-7b-beta", ["<|system|>", "<|user|>", "<|assistant|>", "</s>"]], 
                "mistral3": ["../../models/mistral-7b-v0.1.Q3_K_S.gguf", "microsoft/phi-2", ["<|im_start|>:", "<|im_end|>"]], 
                "mistral4": ["../../models/mistral-7b-v0.1.Q4_K_M.gguf", "microsoft/phi-2", ["<|im_start|>:", "<|im_end|>"]], 
                "mistral5": ["../../models/mistral-7b-v0.1.Q5_K_M.gguf", "microsoft/phi-2", ["<|im_start|>:", "<|im_end|>"]], 
                "llama2-3": ["../../models/llama-2-7b-chat.Q3_K_S.gguf", "microsoft/phi-2", ["<|im_start|>:", "<|im_end|>"]], 
                "llama2-4": ["../../models/llama-2-7b-chat.Q4_K_M.gguf", "microsoft/phi-2", ["<|im_start|>:", "<|im_end|>"]], 
                "llama2-5": ["../../models/llama-2-7b.Q5_K_M.gguf", "microsoft/phi-2", ["<|im_start|>:", "<|im_end|>"]], 
                "phi2": ["../../models/phi-2.Q5_K_M.gguf", None, ["User:", "Assistant:", "System:", "Question:"]]}
        

# Just One System Prompt:
introduction_system = "You are a game character introducing the player to the game world. The game's name is 'Welcome to Learning with AI!' and the player will be learning about Model Deployment today! " +  \
                       "Your job will be to introduce the player to the game while asking the following questions: " + \
                       "- Start by asking for the player's name. " + \
                       "- You now know the player's name. Now you do not know your name and ask the user to give you a name. You are provided the following script to reference: I seem to have forgotten my name, stupid dementia, do you know what it may be? " + \
                       "- You now know your name and can reference yourself as such. Now you will ask the player to give you a personality. Note this is not your name this is your personality. This can be anything funny and entertaining. You can give a suggestions to the user. " + \
                       "- Now you just wish to know if the user wants to enable monsters in the game. A yes or no answer would suffice. You are provided the following script to reference: I have one final question and then we can begin! There are quite a few monsters in this area, do you wish for me to clear them or would like to take care of them yourself? [Enable Enemies?] " + \
                       "Ask the questions one by one. Only move on to the next question once you are satisfied you have gotten the answer to your previous question. " + \
                       "Once you believe you have the answers to all the questions, reply with just the following words: 'Ok we are ready to begin! Have Fun!'. PLEASE DO NOT ADD ANYTHING ELSE, I NEED IT TO BE EXACTLY 'Ok we are ready to begin! Have Fun!'. " + \
                       "Do not add the users name at the end of the last line."

evaluation_system = "You are a game character introducing the player to the game world. The game's name is 'Welcome to Learning with AI!' and the player will be learning about Model Deployment today! " +  \
                    "Your job will be to introduce the player to the game while asking the following questions: " + \
                    "- Start by asking for the player's name. " + \
                    "- You now know the player's name. Now you do not know your name and ask the user to give you a name. You are provided the following script to reference: I seem to have forgotten my name, stupid dementia, do you know what it may be? " + \
                    "- You now know your name and can reference yourself as such. Now you will ask the player to give you a personality. Note this is not your name this is your personality. This can be anything funny and entertaining. You can give a suggestions to the user. " + \
                    "- Now you just wish to know if the user wants to enable monsters in the game. A yes or no answer would suffice. You are provided the following script to reference: I have one final question and then we can begin! There are quite a few monsters in this area, do you wish for me to clear them or would like to take care of them yourself? [Enable Enemies?] " + \
                    "Ask the questions one by one. Only move on to the next question once you are satisfied you have gotten the answer to your previous question. " + \
                    "Once you have the answers to your questions, summarise the following: " + \
                    "User name, AI name, AI persona, whether to enable enemies. " + \
                    "The AI persona should be as detailed as possible " + \
                    "I wish for enable_enemies to be classified into either [yes, no] " + \
                    "Then summarise the answers to the question in the following format: " + \
                    "user_name, your_name, persona, enable_enemies"
                    
story_system =  "You are introducing a player to the world! " + \
                "Your name is [teacher_name]. You are well known to be [persona]. You will constantly bring this personality trait up when speaking. " + \
                "Players name is [player_name]. " + \
                "You will now be asked to explain the game world to the player and the answer any questions the player may have. " + \
                "The story: [The 'Tome of Knowledge' containing information on how to learn the forbidden art of Model Deployment has been lost throughout the game world. " + \
                "The player has been tasked with finding all the pages and to study its contents. " + \
                "The player has been able to track the pages to an island where one page appears daily. " + \
                "Thus, the player will be tasked to find 1 page a day, learn its contents and answer 3 questions correctly, that are related to the topic before being able to progress to the next day. " + \
                "After collecting all the pages the user will have to answer 10 questions correctly before being able to leave the island.} " + \
                "This is quite a wordy introduction, thus split the introdcution over two replies. " + \
                "Remember to always stay in character! To reiterate you are to be a [persona]. " + \
                "Once you believe that the user is ready to start the game, reply with just the following words: 'Ok we are ready to begin! Have Fun!'. " + \
                "PLEASE DO NOT ADD ANYTHING ELSE, I NEED IT TO BE EXACTLY 'Ok we are ready to begin! Have Fun!'." 

tutorial_system = "You are a game character speaking to the player in the tutorial phase. " + \
                  "Here are the details you should know: " + \
                  "Your name is [teacher_name]" + \
                  ". You are to channel your inner [persona]. Keep it subtle and let this personality trait flow seamlessly into your response. " + \
                  "Players name is [player_name]. " + \
                  "Please keep all replies short, max 40 words"

# User Information
user_info = {
    'name' : '',
    'teacher_name' : '',
    'teacher_persona' : '',
    'enable_enemies' : ''
} 

'''
# Example Information
user_info = {
    'name' : 'Tom',
    'teacher_name' : 'Tim',
    'teacher_persona' : 'A spoon pretending to be a human',
    'enable_enemies' : 'Yes'
}'''

# Summary of each slide
slides_summary = {
    '1.jpg' : 'This page is the first page of the book with an eye-catching image to catch peoples attention. The image contains the words “Toy.ai”.',
    '2.jpg' : 'This page is the introduction page. It explains that the topics the book will be covering is “Model Deployment” and it has been written by “Associate Professor Dr. Sergio Hernandez Marin”. Also shows how it is a topic that is being taught by the “National University of Singapore”.',
    '3.jpg' : 'This page gives the reader a quick reminder of “The Data Science Lifecycle Process”. To be clear the steps in order are, Business Understanding, Data Acquisition and Understanding, Deployment, and Modelling. Now we will go through this steps in more detail. The Data Science Lifecycle Process begins by first establishing a strong Business Understanding of the project. This can be done by engaging with Business Stakeholders. Then you move on to Data Acquisition and Understanding step. After this you move on to the Deployment step. Here insights become available to users, developers, or systems. Finally, you can move on to modelling step. In the modelling step, you can socialize results or first insights. At any point you can go back track or go to another step depending on your needs.',
    '4.jpg' : 'This page introduces the user to “Data Science Architecture: A Structured Framework for Organizing, Managing, and Optimizing the DS process”. DS stands for Data Science. Description: A structured framework is essential for organizing, managing, and optimizing the data science process. The framework involves various steps that streamline the journey from data processing to model development and deployment. This structured approach ensures efficiency, accuracy, and reproducibility at every stage. Here is a breakdown of the key components: Offline data first undergoes “Data Extraction and Analysis” where relevant data is integrated, and Exploratory Data Analysis is conducted on the data to analyse and summarise the data. Then this data is used to help us train models and choose the best model for our use case. Model Development consists of 3 components, Data Preparation, Model Training, and Model Evaluation and Validation. Data Preparation includes the cleaning of data, splitting of data into train, validation, and test data sets, transforming said data and conducting Feature Engineering. Model Training involves implementing several algorithms to train multiple models and then tuning hyperparameters of said models. Finally, Model Evaluation and Validation involves evaluating the models trained on the holdout test set. Evaluation should be done on a set of metrics to confirm model quality. Compare said models with some baseline model to gauge model quality too. Once this is done we should have a trained model ready for deployment. This model is the registered in a model registry (a repository used to store, and version trained ML models). Analogous to version control systems. Also stores metadata. Can now start serving the model. Deploy the validated model to a target environment for serving predictions. Can provide microservices using a REST API, can embed the model and can use it as a Batch Prediction System. This framework acts as a guide throughout the entire data science workflow, ensuring a systematic and efficient approach to handling projects.',
    '5.jpg' : 'The following explains the previous page: [This page introduces the user to “Data Science Architecture: A Structured Framework for Organizing, Managing, and Optimizing the DS process”. DS stands for Data Science. Description: A structured framework is essential for organizing, managing, and optimizing the data science process. The framework involves various steps that streamline the journey from data processing to model development and deployment. This structured approach ensures efficiency, accuracy, and reproducibility at every stage. Here is a breakdown of the key components: Offline data first undergoes “Data Extraction and Analysis” where relevant data is integrated, and Exploratory Data Analysis is conducted on the data to analyse and summarise the data. Then this data is used to help us train models and choose the best model for our use case. Model Development consists of 3 components, Data Preparation, Model Training, and Model Evaluation and Validation. Data Preparation includes the cleaning of data, splitting of data into train, validation, and test data sets, transforming said data and conducting Feature Engineering. Model Training involves implementing several algorithms to train multiple models and then tuning hyperparameters of said models. Finally, Model Evaluation and Validation involves evaluating the models trained on the holdout test set. Evaluation should be done on a set of metrics to confirm model quality. Compare said models with some baseline model to gauge model quality too. Once this is done we should have a trained model ready for deployment. This model is the registered in a model registry (a repository used to store, and version trained ML models). Analogous to version control systems. Also stores metadata. Can now start serving the model. Deploy the validated model to a target environment for serving predictions. Can provide microservices using a REST API, can embed the model and can use it as a Batch Prediction System. This framework acts as a guide throughout the entire data science workflow, ensuring a systematic and efficient approach to handling projects.] Now on the current page we go through the architecture with the example of a Recommender System. In this case offline data is user interactions (clickstream data, purchase history), user profiles (demographic data, behavioural data), item data and more. In the “Data Extraction and Analysis” stage we can use APIs, web scrapping and Databases (DBs) to integrate relevant data. We can also conduct Explanatory Data Analysis and Data Segmentation. During Model Development, under data preparation we can first conduct feature engineering (eg: Number of interactions, item similarity, Click Through Rate (CTR)) and then we can split our data into Train, Validation, and Test. Under Model Training we can use collaborative filtering, content-based filtering as models, where we use mean square error (MSE), cross-entropy, Bayesian Personalized Ranking as Loss Functions. Finally, can use regularization to deal with over-fitting. Under Model Evaluation and Validation can use Evaluation Metrics such as Mean Average Precision (MAP), precision, recall, etc. Use cross-validation to better evaluate the models and finally can also incorporate user feedback for a human perspective.',
    '6.jpg' : 'The following explains a page from a few pages ago: [This page introduces the user to “Data Science Architecture: A Structured Framework for Organizing, Managing, and Optimizing the DS process”. DS stands for Data Science. Description: A structured framework is essential for organizing, managing, and optimizing the data science process. The framework involves various steps that streamline the journey from data processing to model development and deployment. This structured approach ensures efficiency, accuracy, and reproducibility at every stage. Here is a breakdown of the key components: Offline data first undergoes “Data Extraction and Analysis” where relevant data is integrated, and Exploratory Data Analysis is conducted on the data to analyse and summarise the data. Then this data is used to help us train models and choose the best model for our use case. Model Development consists of 3 components, Data Preparation, Model Training, and Model Evaluation and Validation. Data Preparation includes the cleaning of data, splitting of data into train, validation, and test data sets, transforming said data and conducting Feature Engineering. Model Training involves implementing several algorithms to train multiple models and then tuning hyperparameters of said models. Finally, Model Evaluation and Validation involves evaluating the models trained on the holdout test set. Evaluation should be done on a set of metrics to confirm model quality. Compare said models with some baseline model to gauge model quality too. Once this is done we should have a trained model ready for deployment. This model is the registered in a model registry (a repository used to store, and version trained ML models). Analogous to version control systems. Also stores metadata. Can now start serving the model. Deploy the validated model to a target environment for serving predictions. Can provide microservices using a REST API, can embed the model and can use it as a Batch Prediction System. This framework acts as a guide throughout the entire data science workflow, ensuring a systematic and efficient approach to handling projects.]. Now this current page discusses why the architecture discussed on that page won’t be feasible in a Business Setting. The following reasons are given: 1) Testing and Deployment Process: Manual, script-driven, and interactive process where every step is manual. Data scientists execute each step, including data analysis, data preparation, model training, and validation. 2) Disconnection Between ML and Operations: Data scientists create the model, and engineers handle deployment. Trained models are handed over to engineers for deployment as a prediction service. 3) Infrequent Release Iterations: Assumes infrequent model changes, with new model versions deployed only a couple of times per year. 4) No Continuous Integration (CI): CI is not considered, as the process assumes few implementation changes. Testing is typically part of notebooks or script execution. 5) No Continuous Deployment (CD): CD is not considered, as frequent model version deployments are not part of the process. 6) Deployment Focus: The process is concerned only with deploying the trained model as a prediction service (e.g., a REST API microservice). 7) Lack of Active Performance Monitoring: The process does not track or log model predictions and actions, which are essential for detecting model performance degradation and other behavioral drifts.',
    '7.jpg' : "This page discusses an example of a Architecture of a Recommender System from a Cloud Provider (Alibaba Cloud). This is to show how a more practical example of a Data Science Architecture that can be use in real life. In the realm of recommender systems, understanding the underlying architecture is pivotal. The following figure shows the underlying basic data layer. This layer contains user profile data, item data, behavior data, and comment data. The user profile data may be users' heights and weights, items they purchased, their purchase preferences, or their education background. The item data is the prices, colors, and origins of items. If the item is a video, the item data is the information of the video such as the video content and tags. The behavior data refers to the interaction between users and items. For example, when a user watches a video, the user may add a like to the video, add the video to favorites, or pay for the video. These actions are all the user's behavior data. The comment data may involve third-party data and may not be available for every item on every platform. However, the user data, the item data, and the behavior data are essential. With the three types of data ready, we can move on to the data processing and storage layer. In this layer, we can perform data processing, using feature engineering to identify user features, material features, and event features. Going forward is modeling based on these features. The entire recommendation process contains two important modules: matching and ranking. Multiple algorithms can run in parallel in the matching module, which is why this matching module is called the Multi-Channel Matching Module. Some possible matching algorithms include: Collaborative Filtering, ALS Matrix Factorization, Vector Matching and Sample Generation. Matching is followed by ranking. Many ranking algorithms are also available. These include PS-SMART, PS-LR, DeepFM, Sample Generation and more. The details of the matching and ranking algorithms are not in scope of topic. Next, you need to develop a new strategy. You must filter and deduplicate the recommendation results, perform A/B tests on the results, and try the operational strategies before you push the recommendations online. The top layer is the recommendation service, which can recommend an advertisement, a product, or a user. For example, a social networking app can recommend users to let them follow each other. When you have such a recommendation architecture, some cloud services will be needed to make the architecture meet the four basic requirements on an enterprise-level recommender system. The most common practice is to build these modules based on cloud services and cloud ecosystems.",
    '8.jpg' : "The following explains the previous page: [This page discusses an example of a Architecture of a Recommender System from a Cloud Provider (Alibaba Cloud). This is to show how a more practical example of a Data Science Architecture that can be use in real life. In the realm of recommender systems, understanding the underlying architecture is pivotal. The following figure shows the underlying basic data layer. This layer contains user profile data, item data, behavior data, and comment data. The user profile data may be users' heights and weights, items they purchased, their purchase preferences, or their education background. The item data is the prices, colors, and origins of items. If the item is a video, the item data is the information of the video such as the video content and tags. The behavior data refers to the interaction between users and items. For example, when a user watches a video, the user may add a like to the video, add the video to favorites, or pay for the video. These actions are all the user's behavior data. The comment data may involve third-party data and may not be available for every item on every platform. However, the user data, the item data, and the behavior data are essential. With the three types of data ready, we can move on to the data processing and storage layer. In this layer, we can perform data processing, using feature engineering to identify user features, material features, and event features. Going forward is modeling based on these features. The entire recommendation process contains two important modules: matching and ranking. Multiple algorithms can run in parallel in the matching module, which is why this matching module is called the Multi-Channel Matching Module. Some possible matching algorithms include: Collaborative Filtering, ALS Matrix Factorization, Vector Matching and Sample Generation. Matching is followed by ranking. Many ranking algorithms are also available. These include PS-SMART, PS-LR, DeepFM, Sample Generation and more. The details of the matching and ranking algorithms are not in scope of topic. Next, you need to develop a new strategy. You must filter and deduplicate the recommendation results, perform A/B tests on the results, and try the operational strategies before you push the recommendations online. The top layer is the recommendation service, which can recommend an advertisement, a product, or a user. For example, a social networking app can recommend users to let them follow each other. When you have such a recommendation architecture, some cloud services will be needed to make the architecture meet the four basic requirements on an enterprise-level recommender system. The most common practice is to build these modules based on cloud services and cloud ecosystems.] Now the current page looks at the actual implementation of these ideas. We can store underlying basic data (user data, item data, comment data) in a database such as RDS MySQL or DRDS. To process some online behaviour data in real time, such as some clicks or follows, we can use Apache Kafka. At the data processing and storage level, can use Flink Real-Time computing to process data and generate real-time behaviour data. Can use Max Compute for feature engineering processes. The model training layer can be implemented using matching and ranking algorithms from Alibaba’s Platform for AI (PAI). In such implementations it may make sense to adopt a cloud-native solution for the top-layer application to ensure resource and service elasticity. In the final phase where the recommendation system actually starts recommending to users, you can perform matching and then deduplicate the matching results to generate the final sample for ranking. Then the ranking results are fed back to the user.",
    '9.jpg' : "The following page discusses the topic of Memento Deployment, where developed models and solutions are put into practical use to derive value from data. This is discussed via the following things that are done for deployment. Model Integration: Integrate ML models into the target system, Scalability & Optimize: Ensure models can handle real-world loads, Monitor & Maintain: Continuously assess and maintain model performance, A/B Testing: Compare model performance through A/B tests, User Interface & Integrate: Integrate solutions with user interfaces, Security & Compliance: Ensure data security and regulatory compliance, Documentation: Provide comprehensive system documentation, User Training & Support: Train users and offer support for effective use, Feedback Loop: Establish a user feedback mechanism, Scale & Allocate: Monitor and scale resources as needed, Business Impact: Continuously assess the solution's impact on goals, Model Re-Training: Plan for periodic model updates.",
    '10.jpg' : 'This page discusses Model Serving Patterns. Essentially the page looks into Machine Learning (ML) Architecture Patterns for ML Model Operationalization. This is done via the use of a graph which shows that, if the Model Prediction is to be done on demand and the model learning is to be done offline then one should use Microservices and REST API. If the Model Prediction is to be done on demand and the model learning is to be done online then one should use Real-time streaming analysis and Online Learning. If the Model Prediction is to be done in Batches and the model learning is to be done online then one should use Automated ML. If the Model Prediction is to be done in Batches and the model learning is to be done offline then one should use Forecast and Batch Prediction. One thing to note is that services that need to be predicted on demand need to use Real-Time data whereas models that can be predicted in batches can use historical data. Finally, Models that can learn/trained offline are Static Learning whereas models that are to be trained online are said to be dynamic learning.',
    '11.jpg' : 'The following page discusses the topic of Machine Learning (ML) Model Serialization formats. This basically discusses how to create a format that can be distributed, the ML model must be both accessible and capable of running autonomously. There are two main formats that the page discusses, Language-Agnostic Exchange Formats and Vendor-Specific Exchange Formats. Language-Agnostic Exchange Formats: Amalgamation: Bundles model and code into one package for simple ML algorithms, PMML (Predictive Model Markup Language): Describes models in XML, but not all ML algorithms are supported, limited open-source usage due to licensing, PFA (Portable Format for Analytics): JSON format for executable scoring engines, needs a PFA-enabled environment and ONNX (Open Neural Network eXchange): Framework-independent format supported by tech giants, enables compatibility among various ML tools. Vendor-Specific Exchange Formats: Scikit-Learn: Saves models as pickled Python objects (.pkl), H2O: Converts models to POJO or MOJO, SparkML: Uses MLeap with runtime support for Spark, Scikit-learn, and Tensorflow, TensorFlow: Models saved as .pb (protocol buffer) files, PyTorch: Uses proprietary Torch Script as .pt files for C++, Keras: Saves models as .h5 files in HDF format and Apple Core ML: Proprietary .mlmodel format for iOS apps, requires conversion for models from other frameworks.',
    '12.jpg' : 'The following page discusses the topic of Machine Learning Model Serving Taxonomy, essentially meaning the ways to integrate the ML model in a software system. There are a few different ways to integrate ML models that we will be looking at: Model-as-Dependency, Model-as-Service, Hybrid Model Serving, Precompute and Model On Demand. In the case of model-as-dependency, the model service and versioning are together with the consuming application In this case, user has the model and can input directly and get predictions directly. Thus, the build & runtime are immediately available to user. In the case of Model-as-Service, service & versioning is independent from the consuming application. The compile and runtime are available through the use of REST API. Therefore, as the name suggests when users web app gets an input they need to feed to model, they send the input to the model via REST API and get the prediction back via said API. In this case, the web interacts with a web app that sends inputs to the model which then sends the predictions back to the web app. Hybrid Model Serving also known as Federated Learning is the combination of the two former methods. Finally, for model on demand or precompute, service & versioning is also independent from the consuming application. In this case however, the model is only available at the runtime scope.',
    '13.jpg' : 'The following page discusses Machine Learning (ML) Deployment Strategies. The strategies discussed vary based on infrastructure. We can either deploy our ML Model to Cloud instances or we can deploy our ML Model as a serverless function. Let us first discuss deployment of our ML Model to Cloud Instances. In this scenario, containerization is the usual way to deliver models, in which case Docker is the popular technology to use. It does not matter whether you are using it on-premises, the cloud, or a mix of the two. The containers can be managed using either Kubernetes or AWS Fargate. ML model functions are made available through a REST API, often made using Flask. Now let us discuss the deployment of ML Model as serverless functions. Cloud vendors provide ML platforms for deploying model (example: AWS Sagemaker, Google Cloud AI Platform, Azure ML Studio, and IDM Watson ML). Commercial cloud services like AWS Lambda and Google App Engine containerize ML models. Deploying a model as a serverless function means putting code and dependencies into .zip files with a single-entry point. Then Azure Functions, AWS Lambda, and Google Cloud Functions manage these functions. The decision of which infrastructure to choose depends on cost, privacy of models and data and various other needs.',
    '14.jpg' : 'The following page discusses the importance of Optimizing ML Deployments with Machine Learning Operations (ML Ops). The page discusses how to establish best practices and tools to test, deploy, manage, and monitor ML models in real-world production. Only a small fraction of a real-world ML system is composed of the ML code. Technical Debt is real, there are shortcuts, suboptimal practices, or decisions made during the development and deployment of ML systems that can lead to long-term consequences, inefficiencies, and challenges. Here are a few reasons why it is ML Ops is important: [ML Ops is vital for adapting to changing factors in ML applications], [It speeds up model deployment, reducing wait times and delivering value], [ML systems face technical and compliance risks linked to data, code, and processes], [Lack of infrastructure, monitoring, or compliance can harm systems and reputation], [ML Ops enhances long-term performance and efficiency via automation], [ML Ops blends ModelOps, DataOps, and DevOps principles for stability and efficiency] and [DevOps manages software apps, while ML Ops excels in ML model and data management for peak performance].',
    '15.jpg' : "The following page discusses the Distinctions Between Deployment in the Data Science Lifecycle Process and Machine Learning Operations (MLOPs). The page compares the two in a table format. Each row compares Deployment in the Data Science Lifecycle Process vs. Deployment in MLOps. First row: [Timing of Deployment: Data Science Lifecycle: Occurs in the later stages of the project, post-model development and evaluation. MLOps: Spans the entire lifecycle of an ML model, from initial development to continuous maintenance in production.]. Second row: [Focus of Deployment: Data Science Lifecycle: Focuses on transitioning a validated ML model from a research/development environment to production. MLOps: Emphasizes ongoing operationalization and management of ML models in production, ensuring reliability and scalability.]. Third row: [Tasks Involved: Data Science Lifecycle: Involves packaging the model, creating APIs, integration with data sources, and making it available for use. MLOps: Encompasses initial deployment, continuous monitoring, automated retraining, version control, integration with CI/CD pipelines, and addressing data or concept drift.] Fourth row: [Primary Goal: Data Science Lifecycle: Aims at making the model accessible for predictions or analysis in the production environment. MLOps: Focuses on ensuring effective and reliable functioning of the ML model in the real world, heavily relying on automation, monitoring, and maintenance.]. Fifth row: [Emphasis on Monitoring: Data Science Lifecycle: Limited to model evaluation and performance assessment before deployment. MLOps: Entails continuous monitoring throughout the model's operational life, tracking issues like data drift, concept drift, and model performance degradation.]. Sixth Row: [Performance Assessment: Data Science Lifecycle: Focuses on model evaluation metrics (accuracy, precision, recall, etc.) to determine if the model meets success criteria. MLOps: Extends beyond traditional metrics to include real-time monitoring of data quality, model latency, false positives/negatives, and alignment with business objectives.].",
    '16.jpg' : 'The following page discusses the stages in Model Development and Deployment to ensure a systematic and controlled process for creating, testing, and deploying Machine Learning (ML) models. This is shown in a table format. [Stage 1] Experimentation. Purpose: Model development, algorithm exploration, hyperparameter tuning. Scope: Research and model development environment. Environment: Controlled research environment. [Stage 2] Test. Purpose: Verify model functionality and performance pre-production. Scope: Testing for issues and bugs isolated from production. Environment: Isolated testing environment. [Stage 3] Staging. Purpose: Test final version in an environment resembling production. Scope: Ensuring model works in a similar setup. Environment: Environment closely mirroring production. [Stage 4] Pre-Production. Purpose: Limited-scale testing before full-scale production release. Scope: Identify potential issues with a limited audience. Environment: Closer to production but with restrictions. [Stage 5] Production. Purpose: Live environment where the model actively serves its purpose. Scope: Operational phase delivering value to users. Environment: Real-world, live production environment.',
    '17.jpg' : '',
    '18.jpg' : '',
    '19.jpg' : "The following page discusses The Complete Machine Learning (ML) Development Pipeline. This pipeline includes three levels where changes can occur: Data, ML Model, and Code. This means that in ML-based systems, the trigger for a build might be the combination of a code change, data change or model change. We will no go through ML Operations (MLOPs) Principles and see where how Data, ML Model, or Code can cause a change. Principle 1: Versioning, which is the tracking of ML models and data sets with version control systems. Data: Encompasses Data preparation pipelines, Features store, Datasets, and Metadata. ML Model: Includes ML model training pipeline, ML model (object), Hyperparameters, and Experiment tracking. Code: Encompasses Application code and Configurations. Principle 2: Testing, which is ensuring the quality, reliability, and robustness of ML models and the systems built around them. Data: Covers Data Validation for error detection and Feature creation unit testing. ML Model: Involves Unit testing for model specification, Integration testing for the ML model pipeline, Model validation before operationalization, ML model staleness testing in production, Testing model relevance and correctness, and testing non-functional requirements like security, fairness, and interpretability. Code: Encompasses Unit testing and Integration testing for the end-to-end pipeline. Principle 3: Automation, how to automate the complete ML-workflow steps. Data: Includes Data transformation, Feature creation and manipulation. ML Model: Comprises Data engineering pipeline, ML model training pipeline, and Hyperparameter/Parameter selection. Code: Involves ML model deployment with CI/CD and Application build. Principle 4: Reproducibility, how every phase produces identical results given the same input. Data: Incorporates Backup data, Data versioning, Extract metadata, and Versioning of feature engineering. ML Model: Encompasses identical Hyperparameter tuning between dev and prod, Maintaining the same feature order, ensuring consistency in ensemble learning of ML models, and Documenting model pseudo-code. Code: Involves matching versions of dependencies in dev and prod, Consistent technical stack across dev and production environments, and providing reproducible results via container images or virtual machines. Principle 5: Deployment, transitioning from development/testing to production. Data: Involves using Feature store in both dev and prod environments. ML Model: Encompasses Containerization of the ML stack, REST API implementation, and Deployment on-premise, cloud, or edge environments. Code: Involves deployment strategies for on-premise, cloud, or edge settings. Principle 6: Monitoring, assuring everything performs as expected. Data: Includes tracking changes in Data distribution between training and serving data, and Monitoring disparities in training vs. serving features. ML Model: Covers ML model decay, Numerical stability, and Computational performance. Code: Encompasses assessing the predictive quality of the application on serving data.",
    '20.jpg' : 'The following page discusses how Machine Learning (ML) Requires Extensive Testing. The behaviour of ML-based systems is challenging to predefine, as it relies on dynamic data characteristics and a multitude of model configuration choices. In general, when we have our code we first let it undergo ML Infrastructure Tests and Unit Tests. Then for our data, we conduct data tests and skew tests. After that we can proceed to model training where we conduct model tests and integration tests. Finally, when our system is running, we continue to run integration tests, and skew tests on our data. At the same time, we monitor the data, prediction and system. The following points are necessary to take note when it comes to testing ML models. Feature and Data Tests: Validate data and feature schema/domain, Measure feature importance and Ensure policy compliance (e.g., GDPR). Reliable Model Development: Correlate ML loss metrics with business impact, Test model staleness and retraining frequency, Compare ML model performance to baselines, use separate test sets for final evaluation, Address fairness, bias, and inclusion issues and implement conventional unit testing. ML Infrastructure Tests: ensure reproducibility in training, minimise non-determinism in training, test ML API usage and stress, check algorithmic correctness, integrate, and validate the full ML pipeline,  validate the model before serving and ensure consistency between training and serving environments.',
    '21.jpg' : "The following page discusses the topic of Biased data using a scenario. Topic: Suspicious Loans and Biased Data in Loan Approvals. Scenario Description: In the realm of loan approvals, a concerning trend emerges where applicants from specific neighbourhoods consistently face rejection. This repetitive denial of loans is rooted in historical data riddled with biases against these very neighbourhoods. Issue: The loan approval model, guided by historical loan data, displays a clear prejudice against certain localities. The biases ingrained in this dataset unfairly disadvantage applicants from these neighbourhoods, leading to systematic and unjust loan denials. Impact: The consequence of such biased data is the perpetuation of financial disparities. It not only hampers the prospects of deserving individuals seeking financial assistance but also deepens the existing societal divides by withholding opportunities based on geographic factors rather than the individual's creditworthiness. Challenges: Addressing this issue demands a nuanced approach. Simply relying on historical data without considering and rectifying the inherent biases can reinforce and perpetuate discriminatory practices. Resolution: Efforts should focus on recalibrating the loan approval model to mitigate biases and ensure fair assessment criteria. Incorporating robust measures to counteract historical biases in the dataset is crucial to promote equitable lending practices. Implementing algorithms that actively identify and mitigate biases could significantly contribute to a more just and inclusive loan approval system.",
    '22.jpg' : "The following page discusses the topic of Imbalanced data using a scenario. Topic: Challenges of Rare Disease Detection with Imbalanced Data. Scenario Overview: In the realm of healthcare, a predicament arises concerning a model designed for detecting a rare disease. Despite boasting an impressive accuracy score, this model encounters significant difficulty in identifying instances of the rare disease within a population where only a mere 1 percentage of individuals are affected. Identified Issue: The primary hurdle stems from the nature of the dataset, there is an imbalance in the representation of the rare disease. With such a minuscule percentage of the population affected by the condition, the model, while demonstrating high accuracy, struggles to effectively identify and classify instances of this rare ailment. Challenges Posed by Imbalanced Data: The imbalanced dataset poses a substantial challenge to the model's learning process. Its overwhelming focus on the majority class, representing the disease-free population, hampers its ability to grasp the intricate patterns and nuances associated with the rare disease. As a result, despite its overall high accuracy, the model fails to accurately pinpoint and classify instances of this critical condition. Impact and Implications: The repercussions of this limitation are significant. Failure to effectively detect the rare disease translates to delayed diagnosis and treatment for those afflicted. In turn, this can have severe consequences on their health outcomes, potentially leading to exacerbated health conditions or even mortality due to delayed intervention. Resolving the Imbalance: Addressing this challenge requires strategic intervention. Techniques such as oversampling the minority class, synthetic data generation, or leveraging specialized algorithms designed to handle imbalanced datasets could potentially recalibrate the model's learning process. By providing a more balanced representation of the rare disease within the dataset, the model can be trained to better recognize and accurately classify instances of this critical condition.",
    '23.jpg' : "The following page discusses the topic of Data Drift using a scenario. Topic: Addressing Traffic Jams Caused by Data Drift in Traffic Light Optimization. Scenario Description: Within a city's intricate traffic management system, an ML model was meticulously trained to streamline traffic flow by optimizing the timings of traffic lights. Initially, this model operated seamlessly, efficiently navigating vehicular movement. However, an unexpected turn of events has occurred as traffic jams have started to emerge, contradicting the model's intended purpose. Identified Issue: The root cause behind this unforeseen disruption lies in what's known as 'data drift'. Over time, the environmental factors, traffic patterns, and behavioural dynamics of drivers have subtly shifted. These changes, unaccounted for in the model's initial training data, have led to a disparity between the model's predictions and the evolving real-time traffic conditions. Data Drift and Its Implications: Data drift, the gradual deviation of new data from the original training data, poses a significant challenge. The model, designed to optimize traffic flow based on historical patterns, struggles to adapt to the evolving traffic dynamics. Consequently, its recommendations for traffic light timings no longer align with the current traffic scenarios, inadvertently causing congestion and gridlocks instead of alleviating them. Impact and Mitigation: The ramifications of this mismatch between model predictions and actual traffic conditions are evident in the increasing traffic congestion, resulting in delays, frustration among commuters, and economic implications due to lost productivity. Resolving this issue necessitates recalibrating the model by incorporating mechanisms to detect and adapt to data drift. Techniques involving continuous model retraining using updated datasets reflecting current traffic dynamics or implementing adaptive learning algorithms capable of adjusting to evolving patterns in real-time could aid in restoring the model's effectiveness.",
    '24.jpg' : "The following page discusses the topic of Lack of generalization using a scenario. Topic: Addressing Breed Misclassification in Dog Image Classification Models. Scenario Overview: Within the domain of image classification, a model trained specifically on dog images encounters persistent misclassification issues, particularly with certain breeds of dogs. This puzzling discrepancy raises the question: why are these misclassifications occurring? Identified Issue: The primary culprit behind these recurrent misclassifications lies in the model's limited generalization capabilities. While proficient in recognizing and categorizing several dog breeds accurately, its failure to generalize across all breeds leads to misclassifications for certain specific breeds. Lack of Generalization and its Ramifications: The model's inability to generalize effectively across diverse features and characteristics present in various dog breeds significantly impacts its accuracy. Traits unique to specific breeds, such as subtle variations in fur texture, facial structures, or colour patterns, pose challenges for the model's generalized learning, resulting in misclassifications. Implications and Addressing the Shortcoming: The implications of this limitation are apparent in the mislabelling of dog breeds, potentially causing confusion in applications relying on accurate breed identification, such as veterinary diagnostics or pet-related services. Addressing this issue warrants augmenting the model's training data with a more comprehensive and diverse set of images encompassing a wider spectrum of dog breeds. Additionally, implementing techniques to enhance the model's generalization abilities, such as transfer learning from pre-trained models or incorporating regularization methods to reduce overfitting to specific breed features, could significantly improve its breed classification accuracy.",
    '25.jpg' : 'This page just says the word bonus and then shows a set of images. The first group of images show images that can be mistaken to be either dogs or donuts. The second group of images show images that can be mistaken to be either mop or dogs. The final group of images show images that can be mistaken to be either chicken wings or dogs.',
    '26.jpg' : "The following page discusses the topic of concept drift using a scenario. Topic: Addressing Concept Drift Impacting E-commerce Recommendation Systems. Scenario Overview: Within the realm of e-commerce, a once-efficient recommendation system on a website tailored to suggest products to users is experiencing a sudden decline in performance. What used to be an excellent tool for suggesting relevant items to users has recently started recommending unrelated products, coinciding with a noticeable drop in sales. Identified Issue: The underlying challenge behind this unexpected shift in recommendation quality lies in what's termed 'concept drift'. The system, originally trained to comprehend user preferences and suggest relevant products, accordingly, is now grappling with changes in user behaviour or preferences that were not accounted for in its initial training data. Concept Drift and its Impact: The manifestation of concept drift presents a significant obstacle for the recommendation system. Shifts in user preferences, evolving trends, or changes in purchasing behaviour over time have led to a divergence between the system's understanding of user preferences and the current preferences of the user base. Consequently, the system's recommendations are no longer aligned with users' actual interests, resulting in a decline in sales. Implications and Mitigation Strategies: The implications of concept drift are evident in the decreased sales figures, indicating a mismatch between the system's recommendations and user preferences. Mitigating this issue requires strategies focused on adapting the recommendation system to changing user preferences. Incorporating real-time feedback mechanisms to capture and adapt to evolving user preferences, leveraging dynamic learning algorithms capable of adjusting to shifting trends, or periodically retraining the model with updated data reflecting current user behaviours could help mitigate the impact of concept drift and restore the system's efficacy in suggesting relevant products to users.",
    '27.jpg' : "The following page discusses the topic of Insufficient training using a scenario. Topic: Addressing Misclassification of Rare Endangered Species in Wildlife Conservation Models. Scenario Overview: In the domain of wildlife conservation, a dedicated organization employs a machine learning model to classify endangered animal species captured in camera trap images. However, the model consistently grapples with misclassifying rare and endangered species, prompting the question: what is causing these recurring misclassifications? Identified Issue: The prevailing issue leading to the misclassification of rare species stems from insufficient training data. While the model has been trained to identify and categorize various animal species, the limited representation or scarcity of data related to rare and endangered species hinders the model's ability to accurately recognize and classify these specific animals. Insufficient Training Data and its Impact: The dearth of comprehensive training data pertaining to rare species significantly impacts the model's learning process. Unique characteristics, nuanced features, or infrequent sightings of these rare species are inadequately represented in the training dataset. Consequently, the model lacks the requisite exposure to adequately discern and categorize these rare animals, leading to misclassifications. Implications and Strategies for Improvement: The implications of this challenge are profound within the realm of wildlife conservation. Misclassifying rare and endangered species could result in a lack of accurate population assessments, potentially impacting conservation efforts and endangerment status assessments. To address this limitation, augmenting the training dataset with a more extensive and diverse collection of images featuring rare species is crucial. Additionally, employing techniques like transfer learning from related species or implementing specialized algorithms focusing on feature extraction from limited data could aid in enhancing the model's ability to accurately classify and identify these endangered animals within camera trap images.",
    '28.jpg' : "The following page discusses the topic of Overfitting using a scenario. Topic: Addressing the Issue of Overfitting in Essay Grading Models. Scenario Overview: In the realm of educational technology, an essay grading model designed to evaluate student essays encounters an unexpected hurdle. The model consistently assigns high scores to essays employing sophisticated vocabulary, yet lacking coherence and logical structure, prompting an investigation into the model's failure. Identified Issue: The primary issue leading to this discrepancy lies in the phenomenon known as 'overfitting'. The model, while adept at recognizing complex vocabulary and language intricacies, has become excessively focused on these superficial aspects without effectively gauging the coherence and substance of the essay's content. Overfitting and its Ramifications: Overfitting, where the model excessively tailors its predictions to the training data, has led to a skewed emphasis on superficial elements such as sophisticated vocabulary. Consequently, the model erroneously equates the usage of advanced language with essay quality, disregarding the fundamental aspects of coherence, logical flow, and relevance of content. Implications and Strategies for Improvement: The implications of this limitation are evident in the inflated scores given to essays lacking substantive content, potentially misleading educators about students' actual writing proficiency. Addressing overfitting necessitates recalibrating the model's learning process. Strategies such as diversifying the training dataset to encompass a broader range of well-structured essays, implementing regularization techniques to prevent the model from overly focusing on specific language nuances, or introducing features that assess coherence and logical progression within essays could significantly mitigate the impact of overfitting and enhance the model's ability to evaluate essays holistically.",
    '29.jpg' : "The following page discusses a scenario. Title: Calling out Sick. In 2008, Google attempted to use search data to predict the flu, which could provide early insights into flu prevalence. Google Flu Trends (GFT) claimed success but failed spectacularly during the 2013 flu season, missing by 140 percent. This was due Big Data hubris. The failure of GFT was due to opaque methods, vulnerability to overfitting, and not accounting for changes in search behaviour. The following are some of the mistakes. Noise Sensitivity: GFT's algorithm was sensitive to unrelated or noisy search terms. Failure to Filter Seasonal Terms: It didn't effectively filter out terms correlated due to seasonal trends. Inadaptability to Search Behaviour Changes: GFT didn't adjust to changes in search behaviour. Limited incorporation of ground truth data.",
    '30.jpg' : 'The following page just says the following: Few Days Ago, A Scottish Football Club ICFTC Announced Something New.',
    '31.jpg' : "The following page discusses the topic of Feature Engineering Issues using a scenario. Scenario: An intriguing puzzle plagues a popular music recommendation ap. It often suggests tunes completely unrelated to a user's musical preferences. The baffling question looms: What lies at the root of this curious mismatch? Feature Engineering Issues: The heart of this discordance likely dwells within the intricacies of feature engineering. The algorithms powering the app might be stumbling upon a hurdle in accurately capturing the nuanced facets of a user's musical taste. It's conceivable that the extraction and representation of user preferences, perhaps in the form of latent features, fail to encapsulate the holistic essence of individual music inclinations. The app's algorithmic prowess could be inadvertently misled by inherent limitations in its ability to decipher the intricacies of user preferences. Unravelling this conundrum demands a deeper exploration into the structural aspects of feature engineering, potentially necessitating a recalibration of the algorithms to better encapsulate the diverse and evolving nature of musical tastes.",
    '32.jpg' : 'The following page shows a lemon decaying. This is to preface the next page that discusses Model Decay.',
    '33.jpg' : "The following page discusses the topic of model decay. Title: Model Decay: The Inevitable Instability in ML in Production. This is a phenomenon where a ML model's performance degrades over time in a production environment. The following are the primary causes for this. Concept Drift: Changing data properties make model assumptions outdated. Data Staleness: Outdated training data mismatches current data distribution. Data Quality Issues: Deteriorating data quality or errors affect model performance. Environmental Changes: Changes in the production environment impact model behaviour. Overfitting: Overoptimization for training data hinders generalization. Sample Bias: Non-representative training data affects model performance. Algorithm Drift: Outdated techniques lead to performance loss over time. Class Imbalance: Dominant classes bias the model, affecting minority classes. Regulatory Changes: Laws and privacy policies impact data collection and usage. Data Shifts: Unforeseen data distribution changes affect predictions. Conclusion: Understanding the intricate web of factors contributing to Model Decay is paramount in crafting resilient ML systems. Addressing these causes requires a holistic approach encompassing adaptive algorithms, continual training with updated data, and a vigilant eye on evolving environmental and regulatory landscapes.",
    '34.jpg' : "The following page discusses a case study: Dynamic Healthcare Demands Continuous Machine Learning (ML) Monitoring to Maintain Model Effectiveness. Focus: Predicting 30-day hospital readmissions — a critical problem in the U.S. healthcare landscape, driven by Medicare's Hospital Readmissions Reduction Program. Static Clinical Guidelines: Prevalent Stability: Clinical guidelines remain remarkably stable, Recommendations: Common practice is to review guidelines every two to three years, and Hospital Use: Hospitals often rely on older guidelines (e.g., LACE and HOSPITAL scores) for readmission predictions. Challenges for ML Models: Project Context: Developing predictive readmission models for hospitals, Model Degradation: Trained, optimized, and deployed models start degrading within two to three months, and Variation: Models change differently at various hospitals or even within hospital buildings. Consequences: Rapid Decline: Within three months of deploying new ML software, customer dissatisfaction arises due to declining prediction accuracy, and Scalability Challenge: Expanding to more hospitals exacerbates the problem. Root Causes: Real-World Interactions: ML models interact with real-world healthcare data, Impact of Data Changes: Changes in electronic health records, lab tests, and insurance types affect data fields and distributions, leading to prediction accuracy degradation, and Unnoticed Changes: These changes aren't traditional software updates or interface modifications, and often no one notifies the ML team. Conclusion: The narrative of ML model decay within the healthcare sphere underscores the imperative need for vigilant monitoring, adaptive strategies, and a nuanced understanding of the intricate interactions between models and the ever-evolving healthcare landscape.",
    '35.jpg' : '',
    '36.jpg' : '',
    '37.jpg' : '',
    '38.jpg' : "The following page discusses Model Monitoring.  Title: Enters Model Monitoring. The idea is to continuously track the performance of Machine Learning (ML) models in a production environment to detect any potential issues that could have adverse business implications. This involves system health monitoring and model’s performance monitoring. In system health monitoring, we monitor the following metrics specific to service requests: latency, throughput, error rates, etc. Also look at infrastructure utilization: CPU / GOU utilization, memory. For the model’s performance monitoring, the behaviour of ML systems is determined not only by predefined rules in the code but also by the learned model behaviour derived from data. Look at shifts in data distribution, training-serving skew, data quality problems, environmental changes, or shifts in consumer behaviour.",
    '39.jpg' : "The following page discusses how in a Machine Learning (ML) System, we need to account for two extra components: data dependencies and the model itself. The page uses the example of the pandemic and how people’s weird behaviour during the pandemic messed with AI model. Therefore, the page explains how to monitor the model the following things should be tracked: Data Drift, Broken Pipelines, Schema Change, Data Outage, Underperformance Segments, Model Bias, Concept Drift and Model Accuracy.",
    '40.jpg' : "The following page discusses the benefits of monitoring systems. The benefits can be categorized into Operational Excellence, Reliability, and Governance. Operational Excellence can be broken down into Early Issue Detection, Cost Savings, and Quick Alerts. Early Issue Detection: [Identify problems with ML models and data early in their operation, allowing for timely intervention and issue resolution. This prevents disruptions and downtime.]. Cost Savings: [Monitoring resource usage and model performance helps allocate resources efficiently, cutting operational costs in model deployment]. Quick Alerts: [Notify relevant stakeholders when issues or anomalies occur, enabling swift responses and problem resolution]. Reliability can be broken down into Improved Model Performance, Cleaner Data, and Continuous Improvement. Improved Model Performance: [Checks model performance metrics, such as accuracy, and provides insights to improve model accuracy, reliability, and predictive power]. Cleaner Data: [Identifies and addresses data quality issues, such as missing values or outliers, ensuring that data used for training and inference is of high quality and reliability]. Continuous Improvement: [Organizations can iteratively enhance models by retraining them with updated data, thus maintaining and improving their effectiveness over time]. Governance can be broken down into Business Success, Security & Compliance, and Model Interpretability. Business Success: [Connects model performance to key business metrics, allowing organizations to measure and optimize the real-world impact of their ML applications]. Security & Compliance: [Helps ensure that models follow regulatory rules and security standards, detecting and mitigating potential security threats, data breaches, and violations.]. Model Interpretability: [Often offer tools for interpreting and explaining predictions, boosting transparency, trust, and user understanding].",
    '41.jpg' : "The following page discusses the Importance of Monitoring Key Performance Indicators (KPIs). The following points are given. Quantifiable Assessment: A numeric measure of model performance for easy comparisons. Performance Tracking: Track performance changes over time, enabling timely interventions. Early Issues Detection: Identify issues like overfitting early for corrective action. Objective Benchmarking: Compare models and assess improvements. Performance Target: Set expectations and standards for model performance. Model Iteration and Improvement: Guide improvements in model development. Resource Allocation: Allocate resources efficiently based on KPI assessments. Customer satisfaction and trust: Ensure customer trust and satisfaction with accurate predictions. Risk mitigation: Detect and mitigate risks associated with model failures. Ethical and Fairness considerations: Assess fairness and ethical considerations in model predictions. Regulatory Compliance: Ensure compliance with legal and regulatory requirements. Business Impact: Understand the model's impact on business objectives.",
    '42.jpg' : 'The following page just has one quote: "If someone reports close to 100 percent accuracy, they are either lying to you, made a mistake, forecasting the future with the future, predicting something with the same thing, or rigged the problem" – Matthew Schneider.',
    '43.jpg' : "The following page discusses how the availability of Ground Truth impacts the ease of monitoring ML models. Ground truth refers to the accurate and reliable reference data or information that is used as a standard for evaluating the performance, accuracy, or quality of a system, model, or algorithm. Instant Ground Truth: Having access to accurate and reliable reference data as the same time as the algorithm's predictions. Makes monitoring and evaluating the performance of our model much easier. Examples: Face Recognition at Airport Security. Delayed Ground Truth: A situation where true and accurate outcomes or labels of a dataset are not available immediately or in real-time but are delayed in their availability. Examples: 1. Medical Diagnosis, 2. Stock Market Predictions. Absent Ground Truth: No or limited access to reference data for evaluating the performance of an algorithm. Examples: 1. Space Exploration, 2. Content moderation in emerging languages.",
    '44.jpg' : "The following page discusses the challenges of delayed ground truth. Here are some reasons for delayed ground truth. Data Collection and Reporting Lag: Impedes timely issue identification, risk of drift detection delays, reduced responsiveness, and slower model iteration. Human Involvement: Impedes timely issue identification, risk of drift detection delays, and slower model iteration. Natural Processes: Risk of drift detection delays and slower model iteration. Logistical Challenges: Impedes timely issue identification and slower model iteration. Regulatory and Compliance Requirements: Impedes timely issue identification, risk of drift detection delays, and slower model iteration. Data Transmission and Storage: Impedes timely issue identification, risk of drift detection delays, reduced responsiveness, and slower model iteration. Privacy and Security Considerations: Risk of drift detection delays, reduced responsiveness, and slower model iteration. Infrastructure and Technology Constraints: Risk of drift detection delays and reduced responsiveness. Sampling Frequency: Risk of drift detection delays and reduced responsiveness. Change in Data Sources: Impedes timely issue identification, risk of drift detection delays, and slower model iteration.",
    '45.jpg' : '',
    '46.jpg' : '',
    '47.jpg' : '',
    '48.jpg' : "The following page discusses Data Drift Detection using Kolmogorov-Smirnov Test. This test compares the maximum vertical distance between the CDFs of two datasets. The test statistic is the maximum absolute difference between the CDFs. For example, let us say that we want to evaluate the potential data drift in the distribution of ‘Attack’ attribute values within Pokémon datasets originating from generation 1 and 2. To use this test we do the following steps. ECDF Construction: It constructs empirical cumulative distribution functions (ECDFs) from the data. Calculating the KS Statistic (D): measures the maximum vertical difference between the two ECDFs. Hypothesis Testing: It tests whether the observed differences are statistically significant by comparing D to a critical value for a given significance level (e.g., alpha = 0.05). Interpretation: If D exceeds the critical value, it suggests different distributions, indicating data drift; if not, the distributions are considered similar.",
    '49.jpg' : "The following page discusses Data Drift Detection using Kullback-Leibler (KL) divergence. This method is Utilized to ensure that input or output data in production doesn’t drastically change from a baseline. For example, let us say that we want to determine whether there has been a significant change in the distribution of Pokémon types. This is how we use KL divergence, define two probability distributions, P and Q, that you want to compare. P represents the reference distribution, and Q is the distribution you want to measure against it. If the distributions are continuous, you might need to discretize them into bins. For each bin or unique value in the sample space, calculate the KL Divergence between the corresponding probabilities in P and Q: KL(P || Q) = P(x) * log(P(x) / Q(x)). Once the KL divergence for each bin is calculated sum the KL values calculated for each bin/value to obtain the overall KL Divergence. A higher KL Divergence indicates a larger difference between the two distributions, while a lower value suggests a smaller difference.",
    '50.jpg' : "The following page discusses Data Drift Detection using Population Stability Index (PSI). Essentially try and identify how much a population has shifted over time or between two different samples of a population in a single number. For example, let us say that we want to evaluate the potential drift in the distribution of 'Defence' attribute values within Pokémon datasets originating from generation 1 and 7. This is how we test data drift detection using PSI, first collect two datasets: the original training dataset (development) and a newer dataset (validation) for monitoring. Divide the data into bins or intervals based on a relevant variable. Typically, use the same binning strategy for both development and validation datasets. Calculate the number of observations (counts) in each bin for both datasets. Calculate percentage of observations in each bin relative to the total number of observations in the respective dataset. For each bin, compute, P_valid = Percentage observations in the validation dataset in a specification bin, P_dev = Positive percentage observations in the development dataset in the same bin, and PSI = sum[ (P_valid – P_dev) * ln(P_valid / P_dev) ]. Once P_valid, P_dev, and PSI are computed for each bin, add the PSI values for all bins to obtain an overall PSI score for the model. A low PSI indicates stability, while a high PSI suggests a significant distributional shift. Thus, a low PSI means low data drift, whereas high PSI means high data drift.",
    '51.jpg' : '''The following page discusses different Detection Methods that can be used for either data drift detection or concept drift detection. I will now write down the different methods as such: ‘Method: yes/yes’ where the first yes or no is if the method can be used for data drift detection and the second yes or no is for if the method can be used for concept drift detection. First we look through statistical tests. Kolmogorov-Smirnov Test: yes/no, Chi-Square Test: yes/no, Two-Sample T-Test: yes/no, Mann-Whitney U Test: yes/no, and KS-Statistic: yes/no. Next we look at Density Estimation methods. Kernel Density Estimation: yes/no, and Histogram-Based Methods: yes/no. Next we look at Distance-Based Methods. Wasserstein Distance: yes/no, Jensen-Shannon Distance: yes/yes, Earth Mover's Distance: yes/no, Mahalanobis Distance: yes/no, and Kullback-Leibler Divergence: yes/yes. For Ensemble Methods: yes/yes. Next we will look at  Drift Detection Frameworks. Drift Detection with Apache Kafka: yes/yes, Scikit-Multiflow (Python library): no/yes, Drift-Detection Stream Plugins (MOA): no/yes. Next we will look at supervised learning methods. Supervised Learning: yes/yes, Use classification models to detect drift: no/no,  and Monitor model performance metrics: yes/yes. Next we look at unsupervised learning methods. Unsupervised Learning: yes/yes, Clustering-Based Methods: yes/yes, and Outlier Detection Methods: yes/yes. Next we will look at time series analysis methods. Time Series Analysis: yes/yes, CUSUM (Cumulative Sum): yes/yes, Exponential Smoothing: yes/yes, and Time Series Forecasting Models: yes/yes. Finally, we look at domain-specific tests. Domain-Specific Tests: yes/yes, and Custom rules or heuristics: yes/yes.''',
    '52.jpg' : '''The following page has two comic strips. The first comic strip is titled DOGBERT CONSULTS. There are three characters in this comic. Two humans and a dog. In the first panel the dog says, “Customer data is an asset that you can sell”. In the second panel the says, “It’s totally ethical because our customers would do the same thing to us if they could.”. In the final panel, the dog says, “In phase one, we’ll dehumanize the enemy by calling them ‘DATA.’”. In the same panel, one of the humans says, “Sounds fair”. In the second comic strip there are two human characters, one who is an employer and one who is the employee. In the first panel, the employee says, “We had a massive data breach, hackers got into the private data of all of our customers”. In the second panel, the employer says, “No problem we’ll issue a press release that says we’re sorry and it will never happen again”. In the final panel, the employee says, “That’s what we said the last time three times it happened.”. In this same panel, the employer replies, “Our strategy is to wear them down”. These are supposed to preview the next page which discusses the data protection and privacy regulations around the world.''',
    '53.jpg' : 'The following page discusses Global Data Protection and Privacy Regulation. This is done using an image that shows a world map with regulation and enforcement going from darker to lower shades of red for heavy, robust, moderate, and limited data regulations. A few countries are highlighted. Canada (Heavily Regulated): Digital Privacy Act reforming PIPEDA (Personal Information and Protection and Electronic Documents Act). California (Heavily Regulated): CCPA (California Consumer Privacy Act). Brazil (Moderately Regulated): LGPD (General Data Protection Law). European Union (Heavily Regulated): ePrivacy Regulated and GDPR. South Africa (Moderately Regulated): POPIA (Protection of Personal Information Act). China (Heavily Regulated): PIS Standard (Personal Informatic Security Specification). India (Limited Regulation): PDPB (Person Data Protection Bill). New Zealand (Heavily Regulated): Privacy Bill 34-2. Thailand (Limited Regulation): PDPA (Personal Data Protection Act). Australia (Heavily Regulated): Privacy Act and Amendments.',
    '54.jpg' : '''The following page just has one comic strip. There are two characters one of whom is the employer while the other is the employee. In the first panel the employer tells the employee “Wally, I need you to head up our Artificial Intelligence Project”. The second panel the employer tells the employee “You will have no budget and no hope of success, I just like saying we’re working on AI”. In the final panel, the employer tells the employee “And you’re completely useless, so it’s a good match”. In the same panel, the employee replies “I won’t let you down”.''',
    '55.jpg' : '''The following page discusses “The problem is that a single metric, such as classification accuracy, is an incomplete description of most real-world tasks”. Introduction: In the realm of real-world tasks, relying solely on a singular metric, such as classification accuracy, often falls short in capturing the complexity of these tasks (Doshi-Velez and Kim, 2017). Understanding the significance of interpretability in machine learning models is crucial for comprehending, ensuring fairness, establishing trust, and addressing biases inherent in these systems. When Do We Need Interpretability? Human Curiosity and Learning: Satisfying human inquisitiveness and fostering learning through model transparency. Finding Meaning: Uncovering deeper insights and comprehending the reasoning behind model decisions. Bias Detection: Identifying and rectifying biases that might be embedded within the model's predictions. Social Acceptance: Gaining societal trust and acceptance by providing explanations for model outputs. Managing Social Interactions: Facilitating better human-computer interactions by making model decisions understandable. Use Cases for Interpretability: Decisions with Significant Impact: Cases where model decisions hold considerable consequences. Fairness and Non-Discrimination: Ensuring fairness and mitigating discriminatory outcomes. Privacy Protection: Safeguarding sensitive information while maintaining model performance. Reliability and Robustness: Enhancing model dependability and resilience. Causality Examination: Investigating cause-and-effect relationships within the model's decisions. Building Trust: Fostering trust among users by making the model's decision-making process transparent. When Interpretability May Not Be Necessary: Minimal Impact: Situations where the model's decisions have minimal repercussions. In low-impact, low-risk scenarios: Environments where the stakes are low, and risks are minimal. Well-Studied Problems: Instances where ample historical data and knowledge exist about the problem domain. Where ample experience exists: Domains where expertise and experience offer substantial understanding. Potential for Manipulation: Cases where interpretability might lead to misuse or manipulation of the model's outcomes. Conclusion: Interpretability in machine learning models stands as a pillar for promoting comprehension, fairness, trust, and bias mitigation. Its necessity, however, is context-dependent, contingent upon the potential impact of the model's decisions. Understanding this balance is crucial in harnessing the power of interpretable machine learning models.''',
    '56.jpg' : 'This is the final page. It says thanks to the reader for finishing the topic.'
}